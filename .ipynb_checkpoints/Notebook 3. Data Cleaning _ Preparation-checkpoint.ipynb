{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EURECOM - Semester Project Fall 2018\n",
    "## Title: Machine Learning in the Kitchen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Data cleaning and preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import csv \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset #1 contains 5500 recipes, the dimentionality of this dataset (number of ingridients) = 3926.  \n",
    "The link to the dataset: https://raw.githubusercontent.com/trungduynguyen/EURECOM-semester-project/master/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "\n",
    "#url_src = 'https://raw.githubusercontent.com/trungduynguyen/EURECOM-semester-project/master/data/'\n",
    "#path = '/home/vlbackend/EURECOM/data/' \n",
    "\n",
    "# download data from github\n",
    "for i in range(11):\n",
    "    url = url_src + 'data' + str(i) + '.csv'\n",
    "    request = urllib.request.Request(url)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    data = response.read()\n",
    "    dest_file = path + 'data' + str(i) + '.csv'\n",
    "    with open(dest_file, 'wb') as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "def load_data():\n",
    "    data_df = []\n",
    "    for i in range(0,11):\n",
    "        temp = pd.DataFrame.from_csv('data/data'+str(i)+'.csv')\n",
    "        data_df.append(temp)\n",
    "        \n",
    "    recipe_df = pd.concat(data_df)  \n",
    "    shuffle_df = shuffle(recipe_df)\n",
    "    data = np.delete(shuffle_df.as_matrix(),[3926],1)\n",
    "    #recipe_labels = shuffle_df.as_matrix()[:,-1] #remove rate\n",
    "    \n",
    "    # list of ingridients\n",
    "    ingridients = list(recipe_df.columns)\n",
    "    ingridients.pop()\n",
    "  \n",
    "    return data,ingridients\n",
    "\n",
    "data,ingridients = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get information about the dataset\n",
    "def get_frequency(matrix,ax):\n",
    "    freq = []\n",
    "    # count number of ingridients for each recipe\n",
    "    if ax == 'row':\n",
    "        for i in range(matrix.shape[0]):\n",
    "            freq.append(np.count_nonzero(matrix[i,:]))\n",
    "    # count number of recipes for each ingridient\n",
    "    if ax == 'column':\n",
    "        for j in range(matrix.shape[1]):\n",
    "            freq.append(np.count_nonzero(matrix[:,j]))\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_ingridients_per_recipe = get_frequency(data, ax='row')\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.title('histogram:'+'\\n'+'number of ingridients per recipes',fontsize=8)\n",
    "plt.grid(True)\n",
    "plt.hist(number_ingridients_per_recipe,bins=40,normed=True,color='salmon',alpha=0.8)\n",
    "plt.show()\n",
    "\n",
    "print('mean:',np.count_nonzero(data)/5500)\n",
    "print('max:',np.max(data),'min:',np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_recipes_per_ingridient = get_frequency(data, ax='column')\n",
    "\n",
    "for i in range(10):\n",
    "    colum_ind = np.where(np.array(number_recipes_per_ingridient) == i)\n",
    "    print(len(colum_ind[0]), 'ingridients take part in', i, 'recipes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2626 ingridients don't take part in any represented recipes, so we delete them from the dataset, and thus significatly reduce the dimentionality of the dataset.  \n",
    "  \n",
    "Since we don't know units of measure of the ingridients in our dataset and users write recipes themselfes using different units, we can convert our real dataset to binary indicationg only the absence or presence of ingridients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete ingridients\n",
    "colum_ind_zero = np.where(np.array(number_recipes_per_ingridient) == 0)\n",
    "cleaned_data = np.delete(data, colum_ind_zero[0], 1)\n",
    "cleaned_ingridients = np.delete(ingridients, colum_ind_zero[0])\n",
    "\n",
    "# convert dataset to binary\n",
    "cleaned_data = (cleaned_data > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in range(1300):\n",
    "    print(x,dict_ingridients[x],\"---\",cleaned_ingridients[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save cleaned data\n",
    "np.savetxt(\"cleaned_data.csv\", cleaned_data, delimiter=\",\")\n",
    "\n",
    "# save cleaned ingridients\n",
    "csvfile = \"cleaned_ingridients.csv\"\n",
    "with open(csvfile, \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in cleaned_ingridients:\n",
    "        writer.writerow([val]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we clean the data more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def import_data(file,num_rec=5500,num_ingr=1300):\n",
    "    data = np.zeros((num_rec,num_ingr))\n",
    "    with open(file) as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            data[i,:] = row\n",
    "            i += 1\n",
    "    return data\n",
    "\n",
    "# import cleaned data\n",
    "data_file ='cleaned_data.csv'\n",
    "cleaned_data = import_data(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import list of cleaned ingridients\n",
    "ingridients_file ='cleaned_ingridients.csv'\n",
    "\n",
    "with open(ingridients_file) as f:\n",
    "    reader = csv.reader(f,delimiter=',')\n",
    "    ingridients = list(reader)\n",
    "cleaned_ingridients = np.squeeze(ingridients)\n",
    "\n",
    "# create dictionary of ingridients\n",
    "dict_ingridients = dict(zip(cleaned_ingridients,range(len(cleaned_ingridients))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_dataset(regex, patterns, cleaned_ingridients, cleaned_data, dict_ingridients):\n",
    "    list_to_delete = []\n",
    "    for x in range(len(cleaned_ingridients)):\n",
    "        if re.search(regex, cleaned_ingridients[x]):\n",
    "            print(x,cleaned_ingridients[x])\n",
    "            for i in range(len(patterns)):\n",
    "                cleaned_ingridients[x] = re.sub(patterns[i],\"\",cleaned_ingridients[x],flags=0)\n",
    "            print(x,cleaned_ingridients[x])\n",
    "            num = dict_ingridients.get(cleaned_ingridients[x],-1)\n",
    "            print('--->',num, cleaned_ingridients[num],\"\\n\")\n",
    "            if num != -1:\n",
    "                cleaned_data[:,num] = cleaned_data[:,num] + cleaned_data[:,x]\n",
    "                list_to_delete.append(x)  \n",
    "\n",
    "    # delete ingridients\n",
    "    cleaned_data = np.delete(cleaned_data, list_to_delete, 1)\n",
    "    cleaned_ingridients = np.delete(cleaned_ingridients, list_to_delete)\n",
    "    print(len(cleaned_ingridients))\n",
    "    print(cleaned_data.shape)\n",
    "    \n",
    "    # create dictionary of ingridients\n",
    "    dict_ingridients = dict(zip(cleaned_ingridients,range(len(cleaned_ingridients))))\n",
    "    return cleaned_ingridients, cleaned_data, dict_ingridients\n",
    "    \n",
    "    \n",
    "regex = r\"to taste\"\n",
    "patterns = [\"(\\sor){0,1}\\sto taste\\s{0,1}\", \"^to taste\\s\"]\n",
    "cleaned_ingridients,cleaned_data,dict_ingridients = clean_dataset(regex,patterns,cleaned_ingridients,cleaned_data,dict_ingridients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regex = r\"or more\"\n",
    "patterns = [\"\\s(or)\\s(more)\"]\n",
    "cleaned_ingridients,cleaned_data,dict_ingridients = clean_dataset(regex,patterns,cleaned_ingridients,cleaned_data,dict_ingridients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regex = r\"^boneless\"\n",
    "patterns = [\"^boneless\\s\"]\n",
    "cleaned_ingridients,cleaned_data,dict_ingridients = clean_dataset(regex,patterns,cleaned_ingridients,cleaned_data,dict_ingridients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = r\"^baby\"\n",
    "patterns = [\"^baby\\s\"]\n",
    "cleaned_ingridients,cleaned_data,dict_ingridients = clean_dataset(regex,patterns,cleaned_ingridients,cleaned_data,dict_ingridients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regex = r\"^whole\\s\"\n",
    "patterns = [\"^whole\\s\"]\n",
    "cleaned_ingridients,cleaned_data,dict_ingridients = clean_dataset(regex,patterns,cleaned_ingridients,cleaned_data,dict_ingridients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = r\"^fresh.*\"\n",
    "patterns = [\"^fresh\\s\"]\n",
    "cleaned_ingridients,cleaned_data,dict_ingridients = clean_dataset(regex,patterns,cleaned_ingridients,cleaned_data,dict_ingridients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regex = r\".*concentrate?\"\n",
    "patterns = [\"\\sconcentrate?\"]\n",
    "cleaned_ingridients,cleaned_data,dict_ingridients = clean_dataset(regex,patterns,cleaned_ingridients,cleaned_data,dict_ingridients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert dataset to binary\n",
    "cleaned_data = (cleaned_data > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain dataset with 1215 ingridients!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save cleaned data\n",
    "np.savetxt(\"new_cleaned_data.csv\", cleaned_data, delimiter=\",\")\n",
    "\n",
    "# save cleaned ingridients\n",
    "csvfile = \"new_cleaned_ingridients.csv\"\n",
    "with open(csvfile, \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in cleaned_ingridients:\n",
    "        writer.writerow([val]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_data(file,num_rec=4300,num_ingr=1526):\n",
    "    data = np.zeros((num_rec,num_ingr))\n",
    "    with open(file) as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            data[i,:] = row\n",
    "            i += 1\n",
    "    return data\n",
    "\n",
    "# import data\n",
    "data_file ='SemesterProject-master/data/data.csv'\n",
    "#data = pd.DataFrame.from_csv(data_file)\n",
    "data = import_data(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(data)/4300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
